{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('C:/Users/aasth/Desktop/Engg of Big Data/Final Project/Archive/yelp_academic_dataset_business.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Splitter(object):\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "class POSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        #adapt format\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Mr', 'Hoagie', 'is', 'an', 'institution', '.'], ['Walking', 'in', ',', 'it', 'does', 'seem', 'like', 'a', 'throwback', 'to', '30', 'years', 'ago', ',', 'old', 'fashioned', 'menu', 'board', ',', 'booths', 'out', 'of', 'the', '70s', ',', 'and', 'a', 'large', 'selection', 'of', 'food', '.'], ['Their', 'speciality', 'is', 'the', 'Italian', 'Hoagie', ',', 'and', 'it', 'is', 'voted', 'the', 'best', 'in', 'the', 'area', 'year', 'after', 'year', '.'], ['I', 'usually', 'order', 'the', 'burger', ',', 'while', 'the', 'patties', 'are', 'obviously', 'cooked', 'from', 'frozen', ',', 'all', 'of', 'the', 'other', 'ingredients', 'are', 'very', 'fresh', '.'], ['Overall', ',', 'its', 'a', 'good', 'alternative', 'to', 'Subway', ',', 'which', 'is', 'down', 'the', 'road', '.']]\n",
      "[[('Mr', 'Mr', ['NNP']), ('Hoagie', 'Hoagie', ['NNP']), ('is', 'is', ['VBZ']), ('an', 'an', ['DT']), ('institution', 'institution', ['NN']), ('.', '.', ['.'])], [('Walking', 'Walking', ['VBG']), ('in', 'in', ['IN']), (',', ',', [',']), ('it', 'it', ['PRP']), ('does', 'does', ['VBZ']), ('seem', 'seem', ['VB']), ('like', 'like', ['IN']), ('a', 'a', ['DT']), ('throwback', 'throwback', ['NN']), ('to', 'to', ['TO']), ('30', '30', ['CD']), ('years', 'years', ['NNS']), ('ago', 'ago', ['RB']), (',', ',', [',']), ('old', 'old', ['JJ']), ('fashioned', 'fashioned', ['VBN']), ('menu', 'menu', ['JJS']), ('board', 'board', ['NN']), (',', ',', [',']), ('booths', 'booths', ['VBZ']), ('out', 'out', ['IN']), ('of', 'of', ['IN']), ('the', 'the', ['DT']), ('70s', '70s', ['CD']), (',', ',', [',']), ('and', 'and', ['CC']), ('a', 'a', ['DT']), ('large', 'large', ['JJ']), ('selection', 'selection', ['NN']), ('of', 'of', ['IN']), ('food', 'food', ['NN']), ('.', '.', ['.'])], [('Their', 'Their', ['PRP$']), ('speciality', 'speciality', ['NN']), ('is', 'is', ['VBZ']), ('the', 'the', ['DT']), ('Italian', 'Italian', ['JJ']), ('Hoagie', 'Hoagie', ['NNP']), (',', ',', [',']), ('and', 'and', ['CC']), ('it', 'it', ['PRP']), ('is', 'is', ['VBZ']), ('voted', 'voted', ['VBN']), ('the', 'the', ['DT']), ('best', 'best', ['JJS']), ('in', 'in', ['IN']), ('the', 'the', ['DT']), ('area', 'area', ['NN']), ('year', 'year', ['NN']), ('after', 'after', ['IN']), ('year', 'year', ['NN']), ('.', '.', ['.'])], [('I', 'I', ['PRP']), ('usually', 'usually', ['RB']), ('order', 'order', ['NN']), ('the', 'the', ['DT']), ('burger', 'burger', ['NN']), (',', ',', [',']), ('while', 'while', ['IN']), ('the', 'the', ['DT']), ('patties', 'patties', ['NNS']), ('are', 'are', ['VBP']), ('obviously', 'obviously', ['RB']), ('cooked', 'cooked', ['VBN']), ('from', 'from', ['IN']), ('frozen', 'frozen', ['JJ']), (',', ',', [',']), ('all', 'all', ['DT']), ('of', 'of', ['IN']), ('the', 'the', ['DT']), ('other', 'other', ['JJ']), ('ingredients', 'ingredients', ['NNS']), ('are', 'are', ['VBP']), ('very', 'very', ['RB']), ('fresh', 'fresh', ['JJ']), ('.', '.', ['.'])], [('Overall', 'Overall', ['JJ']), (',', ',', [',']), ('its', 'its', ['PRP$']), ('a', 'a', ['DT']), ('good', 'good', ['JJ']), ('alternative', 'alternative', ['NN']), ('to', 'to', ['TO']), ('Subway', 'Subway', ['NNP']), (',', ',', [',']), ('which', 'which', ['WDT']), ('is', 'is', ['VBZ']), ('down', 'down', ['IN']), ('the', 'the', ['DT']), ('road', 'road', ['NN']), ('.', '.', ['.'])]]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Mr Hoagie is an institution. Walking in, it does seem like a throwback to 30 years ago, old fashioned menu board, booths out of the 70s, and a large selection of food. Their speciality is the Italian Hoagie, and it is voted the best in the area year after year. I usually order the burger, while the patties are obviously cooked from frozen, all of the other ingredients are very fresh. Overall, its a good alternative to Subway, which is down the road.\"\"\"\n",
    "\n",
    "splitter = Splitter()\n",
    "postagger = POSTagger()\n",
    "\n",
    "splitted_sentences = splitter.split(text)\n",
    "\n",
    "print(splitted_sentences)\n",
    "\n",
    "pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "\n",
    "print(pos_tagged_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DictionaryTagger(object):\n",
    "    def __init__(self, dictionary_paths):\n",
    "        files = [open(path, 'r') for path in dictionary_paths]\n",
    "        dictionaries = [yaml.load(dict_file) for dict_file in files]\n",
    "        map(lambda x: x.close(), files)\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                if key in self.dictionary:\n",
    "                    self.dictionary[key].extend(curr_dict[key])\n",
    "                else:\n",
    "                    self.dictionary[key] = curr_dict[key]\n",
    "                    self.max_key_size = max(self.max_key_size, len(key))\n",
    "\n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=False):\n",
    "        \"\"\"\n",
    "        the result is only one tagging of all the possible ones.\n",
    "        The resulting tagging is determined by these two priority rules:\n",
    "            - longest matches have higher priority\n",
    "            - search is made from left to right\n",
    "        \"\"\"\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N) #avoid overflow\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1] for word in sentence[i:j]]).lower()\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "                if literal in self.dictionary:\n",
    "                    #self.logger.debug(\"found: %s\" % literal)\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "                    if is_single_token: #if the tagged literal is a single token, conserve its previous taggings:\n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "global yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Mr', 'Mr', ['NNP']), ('Hoagie', 'Hoagie', ['NNP']), ('is', 'is', ['VBZ']), ('an', 'an', ['DT']), ('institution', 'institution', ['NN']), ('.', '.', ['.'])], [('Walking', 'Walking', ['VBG']), ('in', 'in', ['IN']), (',', ',', [',']), ('it', 'it', ['PRP']), ('does', 'does', ['VBZ']), ('seem', 'seem', ['VB']), ('like', 'like', ['IN']), ('a', 'a', ['DT']), ('throwback', 'throwback', ['NN']), ('to', 'to', ['TO']), ('30', '30', ['CD']), ('years', 'years', ['NNS']), ('ago', 'ago', ['RB']), (',', ',', [',']), ('old', 'old', ['JJ']), ('fashioned', 'fashioned', ['VBN']), ('menu', 'menu', ['JJS']), ('board', 'board', ['NN']), (',', ',', [',']), ('booths', 'booths', ['VBZ']), ('out', 'out', ['IN']), ('of', 'of', ['IN']), ('the', 'the', ['DT']), ('70s', '70s', ['CD']), (',', ',', [',']), ('and', 'and', ['CC']), ('a', 'a', ['DT']), ('large', 'large', ['JJ']), ('selection', 'selection', ['NN']), ('of', 'of', ['IN']), ('food', 'food', ['NN']), ('.', '.', ['.'])], [('Their', 'Their', ['PRP$']), ('speciality', 'speciality', ['NN']), ('is', 'is', ['VBZ']), ('the', 'the', ['DT']), ('Italian', 'Italian', ['JJ']), ('Hoagie', 'Hoagie', ['NNP']), (',', ',', [',']), ('and', 'and', ['CC']), ('it', 'it', ['PRP']), ('is', 'is', ['VBZ']), ('voted', 'voted', ['VBN']), ('the', 'the', ['DT']), ('best', 'best', ['JJS']), ('in', 'in', ['IN']), ('the', 'the', ['DT']), ('area', 'area', ['NN']), ('year', 'year', ['NN']), ('after', 'after', ['IN']), ('year', 'year', ['NN']), ('.', '.', ['.'])], [('I', 'I', ['PRP']), ('usually', 'usually', ['RB']), ('order', 'order', ['NN']), ('the', 'the', ['DT']), ('burger', 'burger', ['NN']), (',', ',', [',']), ('while', 'while', ['IN']), ('the', 'the', ['DT']), ('patties', 'patties', ['NNS']), ('are', 'are', ['VBP']), ('obviously', 'obviously', ['RB']), ('cooked', 'cooked', ['VBN']), ('from', 'from', ['IN']), ('frozen', 'frozen', ['JJ']), (',', ',', [',']), ('all', 'all', ['DT']), ('of', 'of', ['IN']), ('the', 'the', ['DT']), ('other', 'other', ['JJ']), ('ingredients', 'ingredients', ['NNS']), ('are', 'are', ['VBP']), ('very', 'very', ['RB']), ('fresh', 'fresh', ['JJ']), ('.', '.', ['.'])], [('Overall', 'Overall', ['JJ']), (',', ',', [',']), ('its', 'its', ['PRP$']), ('a', 'a', ['DT']), ('good', 'good', ['positive', 'JJ']), ('alternative', 'alternative', ['NN']), ('to', 'to', ['TO']), ('Subway', 'Subway', ['NNP']), (',', ',', [',']), ('which', 'which', ['WDT']), ('is', 'is', ['VBZ']), ('down', 'down', ['IN']), ('the', 'the', ['DT']), ('road', 'road', ['NN']), ('.', '.', ['.'])]]\n"
     ]
    }
   ],
   "source": [
    "dicttagger = DictionaryTagger([ 'C:/Users/aasth/Desktop/dicts/positive.yml', 'C:/Users/aasth/Desktop/dicts/negative.yml'])\n",
    "\n",
    "dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "\n",
    "print(dict_tagged_sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-028744a2747d>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-27-028744a2747d>\"\u001b[1;36m, line \u001b[1;32m21\u001b[0m\n\u001b[1;33m    afinn = dict(map(lambda (w, s): (w, int(s)), [\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python \n",
    "#\n",
    "# (originally entered at https://gist.github.com/1035399)\n",
    "#\n",
    "# License: GPLv3\n",
    "#\n",
    "# To download the AFINN word list do:\n",
    "# wget http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/6010/zip/imm6010.zip\n",
    "# unzip imm6010.zip\n",
    "#\n",
    "# Note that for pedagogic reasons there is a UNICODE/UTF-8 error in the code.\n",
    "\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "reload(sys)\n",
    "\n",
    "\n",
    "# AFINN-111 is as of June 2011 the most recent version of AFINN\n",
    "filenameAFINN = 'AFINN/AFINN-111.txt'\n",
    "afinn = dict(map(lambda (w, s): (w, int(s)), [ \n",
    "            ws.strip().split('\\t') for ws in open(filenameAFINN) ]))\n",
    "\n",
    "# Word splitter pattern\n",
    "pattern_split = re.compile(r\"\\W+\")\n",
    "\n",
    "def sentiment(text):\n",
    "    \"\"\"\n",
    "    Returns a float for sentiment strength based on the input text.\n",
    "    Positive values are positive valence, negative value are negative valence. \n",
    "    \"\"\"\n",
    "    words = pattern_split.split(text.lower())\n",
    "    sentiments = map(lambda word: afinn.get(word, 0), words)\n",
    "    if sentiments:\n",
    "        # How should you weight the individual word sentiments? \n",
    "        # You could do N, sqrt(N) or 1 for example. Here I use sqrt(N)\n",
    "        sentiment = float(sum(sentiments))/math.sqrt(len(sentiments))\n",
    "        \n",
    "    else:\n",
    "        sentiment = 0\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Single sentence example:\n",
    "    text = \"Finn is stupid and idiotic\"\n",
    "    print(\"%6.2f %s\" % (sentiment(text), text))\n",
    "    \n",
    "    # No negation and booster words handled in this approach\n",
    "    text = \"Finn is only a tiny bit stupid and not idiotic\"\n",
    "    print(\"%6.2f %s\" % (sentiment(text), text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_of(sentiment):\n",
    "    if sentiment == 'positive': return 1\n",
    "    if sentiment == 'negative': return -1\n",
    "    return 0\n",
    "\n",
    "def sentiment_score(review):    \n",
    "    return sum ([value_of(tag) for sentence in dict_tagged_sentences for token in sentence for tag in token[2]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_score(dict_tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
